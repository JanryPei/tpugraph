{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom typing import Dict, Optional, List, Union, Tuple\nfrom dataclasses import dataclass\nimport math\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import  DataLoader\n\nfrom transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\nfrom transformers.pytorch_utils import apply_chunking_to_forward\nfrom transformers.activations import ACT2FN\nimport pytorch_lightning as pl\nimport torchmetrics as tm\n# import bitsandbytes as bnb","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:31.151302Z","iopub.execute_input":"2023-09-28T12:14:31.151864Z","iopub.status.idle":"2023-09-28T12:14:49.30398Z","shell.execute_reply.started":"2023-09-28T12:14:31.151826Z","shell.execute_reply":"2023-09-28T12:14:49.302654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NODE_OP_CODES = 120\nNODE_FEATS = 140\nCONFIG_FEATS = 24\nNODE_CONFIG_FEATS = 18","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:49.310097Z","iopub.execute_input":"2023-09-28T12:14:49.310513Z","iopub.status.idle":"2023-09-28T12:14:49.320238Z","shell.execute_reply.started":"2023-09-28T12:14:49.310474Z","shell.execute_reply":"2023-09-28T12:14:49.319181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"../input/predict-ai-model-runtime/npz_all/npz\"\n\n\ndef generate_tile_df() -> pd.DataFrame:\n    tile_df = pd.DataFrame({'paths': [elem for elem in (Path(DATA_DIR) / 'tile').rglob(\"*\") if elem.is_file()]}).assign(\n        split=lambda df: df.paths.apply(lambda x: x.parent.name),\n        configuration=lambda df: df.paths.apply(lambda x: x.parent.parent.name),\n        extra=lambda df: df.paths.apply(lambda x: x.parent.parent.parent.name),\n        model_name=lambda df: df.paths.apply(lambda x: x.stem),\n        collection=lambda df: df.extra + ':' + df.configuration ,\n        ID=lambda df: df.collection + ':' + df.model_name ,\n        paths = lambda df: df.paths.apply(lambda x: str(x))\n    )\n    return tile_df","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:49.325866Z","iopub.execute_input":"2023-09-28T12:14:49.329019Z","iopub.status.idle":"2023-09-28T12:14:49.342504Z","shell.execute_reply.started":"2023-09-28T12:14:49.328979Z","shell.execute_reply":"2023-09-28T12:14:49.341045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tile_df = generate_tile_df()\ntile_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:49.350261Z","iopub.execute_input":"2023-09-28T12:14:49.352887Z","iopub.status.idle":"2023-09-28T12:14:58.932675Z","shell.execute_reply.started":"2023-09-28T12:14:49.352851Z","shell.execute_reply":"2023-09-28T12:14:58.931651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n* Create an Adjacency matrix for masking the attention\n* Creates a virtual first node equivalent to the [CLS] token which contains the global config for tile cases, while layout node configuration goes to the corresponding node position","metadata":{}},{"cell_type":"code","source":"def edges_adjacency(edges: torch.Tensor, add_diagonal=True) -> torch.Tensor:\n    \"\"\"\n    Generate an adjacency matrix from the edges\n    Args:\n        edges: Tensor of shape (num_edges, 2) with the edges\n        add_diagonal: Boolean indicating if the diagonal should be added to the adjacency matrix\n    Returns:\n        adjacency_matrix: Tensor of shape (num_nodes, num_nodes) with the adjacency matrix\n    \"\"\"\n    adjacency_matrix = torch.zeros((edges.max() + 1, edges.max() + 1))\n    adjacency_matrix[edges[:, 0], edges[:, 1]] = 1\n    if add_diagonal:\n        diag_idx = torch.arange(adjacency_matrix.shape[0])\n        adjacency_matrix[diag_idx, diag_idx] = 1\n    return adjacency_matrix\n\ndef tile_loader(path):\n    tile_dict =  dict(np.load(path))\n    tile_dict = {k: torch.from_numpy(v) for k, v in tile_dict.items()}\n    tile_dict['edges_adjecency'] = edges_adjacency(tile_dict['edge_index'])\n    return tile_dict\n\ndef node_cls_token(elem_dict, shift_node_config_ids:bool=True):\n    \"\"\"\n    Add a cls token to the node opcode, features, edges adjacency matrix, shift node_config_ids by 1 to account for the cls token\n    Args:\n        elem_dict: Dictionary with the elements of the tile\n    Returns:\n        elem_dict: Dictionary with the elements of the tile with the cls token\n    \"\"\"\n    elem_dict['node_opcode'] = torch.cat([torch.tensor([0]), elem_dict['node_opcode']]) # Introduce [CLS] node\n    elem_dict['node_feat'] = torch.cat([torch.zeros((1, elem_dict['node_feat'].shape[1])), elem_dict['node_feat']])\n    elem_dict['edges_adjecency'] = F.pad(elem_dict['edges_adjecency'], (1,0,1,0), value=1)\n    if 'node_config_ids' in elem_dict and shift_node_config_ids:\n        elem_dict['node_config_ids'] = elem_dict['node_config_ids'] + 1 # Shift Node Config IDs to take in to account [CLS] node\n    return elem_dict\n\n\nclass TileDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, df:pd.DataFrame ,add_cls_token:bool=True, num_configs:int=10,  max_configs:Optional[int]=None):\n        self.df = df\n        self.add_cls_token = add_cls_token\n        self.num_configs = num_configs\n        self.max_configs = max_configs  \n        \n    def __len__(self) -> int:\n        return len(self.df)\n    \n    def select_configs(self, total_configs:int):\n        if self.max_configs is not None:\n            total_configs = min(total_configs, self.max_configs)\n        if self.num_configs == -1:\n            return np.arange(total_configs)\n        if total_configs < self.num_configs:\n            return np.random.choice(total_configs, self.num_configs, replace=True)\n        return  np.random.choice(total_configs, self.num_configs, replace=False)\n    \n    def __getitem__(self, idx:int, selected_configs:List[int]=None):\n        tile_dict = tile_loader(self.df.paths[idx])\n        if selected_configs is None:\n            selected_configs = self.select_configs(tile_dict['config_feat'].shape[0])\n        tile_dict['node_config_feat'] = tile_dict.pop('config_feat')[selected_configs]\n        tile_dict['node_config_feat'] = F.pad(tile_dict['node_config_feat'].unsqueeze(1), (0,NODE_CONFIG_FEATS))\n        tile_dict['config_runtime'] = tile_dict['config_runtime'][selected_configs].float()\n        tile_dict['config_runtime'] /= tile_dict['config_runtime_normalizers'][selected_configs].float()\n        tile_dict['node_config_ids'] = torch.zeros((1,))\n        tile_dict['selected_idxs'] = selected_configs\n        if self.add_cls_token:\n            tile_dict = node_cls_token(tile_dict, False)\n        return tile_dict","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:58.936258Z","iopub.execute_input":"2023-09-28T12:14:58.936567Z","iopub.status.idle":"2023-09-28T12:14:58.956189Z","shell.execute_reply.started":"2023-09-28T12:14:58.936542Z","shell.execute_reply":"2023-09-28T12:14:58.954985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tile_dataset = TileDataset(tile_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:58.95768Z","iopub.execute_input":"2023-09-28T12:14:58.958272Z","iopub.status.idle":"2023-09-28T12:14:58.983309Z","shell.execute_reply.started":"2023-09-28T12:14:58.958233Z","shell.execute_reply":"2023-09-28T12:14:58.982249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elem = tile_dataset[0]\nfor k,v in elem.items():\n    print(k, v.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:58.984962Z","iopub.execute_input":"2023-09-28T12:14:58.985353Z","iopub.status.idle":"2023-09-28T12:14:59.088012Z","shell.execute_reply.started":"2023-09-28T12:14:58.985294Z","shell.execute_reply":"2023-09-28T12:14:59.086608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elem['edges_adjecency']","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.089463Z","iopub.execute_input":"2023-09-28T12:14:59.089859Z","iopub.status.idle":"2023-09-28T12:14:59.127158Z","shell.execute_reply.started":"2023-09-28T12:14:59.089825Z","shell.execute_reply":"2023-09-28T12:14:59.126167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Collator","metadata":{}},{"cell_type":"code","source":"def pad_edge_adjacency(edges_adjacency_list):\n    max_len = max([elem.shape[0] for elem in edges_adjacency_list])\n    return torch.stack([F.pad(elem, (0, max_len-elem.shape[0], 0, max_len-elem.shape[0]), value=0) for elem in edges_adjacency_list], dim=0)\n\n@dataclass\nclass LayoutCollator:\n    pad_to_multiple_of: int = 64\n    targets:bool = True\n    padding_idx:int = 120\n    node_padding_idx:int = 0\n    \n    def __call__(self, batch):\n        output = {}\n        max_node_len = max([elem['node_opcode'].shape[0] for elem in batch])\n        node_pad_amount = self.pad_to_multiple_of - max_node_len % max(self.pad_to_multiple_of, 1)\n        output['node_opcode'] = F.pad(pad_sequence([elem['node_opcode'] for elem in batch], batch_first=True, padding_value=self.padding_idx),\n                                      (0, node_pad_amount), value=self.padding_idx).long()\n        output['node_feat'] = F.pad(pad_sequence([elem['node_feat'] for elem in batch], batch_first=True),\n                                    (0,0,0, node_pad_amount), value=0)\n        output['edges_adjecency'] = F.pad(pad_edge_adjacency([elem['edges_adjecency'] for elem in batch]),\n                                          (0, node_pad_amount, 0, node_pad_amount), value=0)\n        output['node_attn_mask'] = F.pad(pad_sequence([torch.ones(len(elem['node_opcode'])) for elem in batch], batch_first=True),\n                                         (0, node_pad_amount), value=0)\n\n        max_node_config_len = max([elem['node_config_ids'].shape[0] for elem in batch])\n        node_config_pad_amount = self.pad_to_multiple_of - max_node_config_len % max(self.pad_to_multiple_of, 1)\n        output['node_config_ids'] = F.pad(pad_sequence([elem['node_config_ids'] for elem in batch], batch_first=True),\n                                         (0, node_config_pad_amount), value=0).long()\n        padded_node_config_feat = pad_sequence([elem['node_config_feat'].permute(1,0,2) for elem in batch], batch_first=True, padding_value=-1)\n        padded_node_config_feat = F.pad(padded_node_config_feat.permute(0,2,1,3),\n                                           (0,0,0, node_config_pad_amount,0,0), value=-1)\n        \n        output['node_config_feat'] = torch.where(padded_node_config_feat!=-1, padded_node_config_feat, self.node_padding_idx)\n                                      \n        output['config_idxs'] = torch.stack([torch.from_numpy(elem['selected_idxs']) for elem in batch])\n        \n        if self.targets:\n            output['config_runtime'] = pad_sequence([elem['config_runtime'].float() for elem in batch], batch_first=True)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.131962Z","iopub.execute_input":"2023-09-28T12:14:59.132773Z","iopub.status.idle":"2023-09-28T12:14:59.162425Z","shell.execute_reply.started":"2023-09-28T12:14:59.132713Z","shell.execute_reply":"2023-09-28T12:14:59.161274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collate_fn = LayoutCollator(64)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.164105Z","iopub.execute_input":"2023-09-28T12:14:59.164556Z","iopub.status.idle":"2023-09-28T12:14:59.176322Z","shell.execute_reply.started":"2023-09-28T12:14:59.164449Z","shell.execute_reply":"2023-09-28T12:14:59.175271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = collate_fn([tile_dataset[0], tile_dataset[1]])\nfor k,v in batch.items():\n    print(k,v.shape)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.182338Z","iopub.execute_input":"2023-09-28T12:14:59.182595Z","iopub.status.idle":"2023-09-28T12:14:59.211291Z","shell.execute_reply.started":"2023-09-28T12:14:59.182572Z","shell.execute_reply":"2023-09-28T12:14:59.210373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model - Config","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass GraphConfig:\n    num_hidden_layers: int = 8\n    hidden_size: int = 256\n    num_attention_heads: int = 16\n    intermediate_size: int = 64\n    chunk_size_feed_forward: int = 64\n    attention_probs_dropout_prob: float = 0.0\n    max_position_embeddings: int = 512\n    hidden_dropout_prob: float = 0.0\n    layer_norm_eps: float = 1e-12\n    hidden_act: str = 'gelu'\n    initializer_range: float = 0.02\n    output_hidden_states: bool = False\n    output_attentions: bool = False\n    gradient_checkpointing: bool = False\n    margin: float = 0.1\n    number_permutations: int = 10\n    \n    def __post_init__(self):\n        self.embedding_size = self.hidden_size\n    \n    def validate(self):\n        if self.hidden_size % self.num_attention_heads != 0 and not hasattr(self, \"embedding_size\"):\n            raise ValueError(\n                f\"The hidden size ({self.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({self.num_attention_heads})\"\n            )\n            \n    def save_config(self, path):\n        config = asdict(self)\n        with open(path, 'w') as f:\n            json.dump(config, f)\n            \n    @classmethod\n    def load_config(cls, path):\n        with open(path, 'r') as f:\n            config = json.load(f)\n        return cls(**config)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.212557Z","iopub.execute_input":"2023-09-28T12:14:59.212881Z","iopub.status.idle":"2023-09-28T12:14:59.226675Z","shell.execute_reply.started":"2023-09-28T12:14:59.212851Z","shell.execute_reply":"2023-09-28T12:14:59.225723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss\n* Uses Ranking loss to compare different configuration\n* Compares does configurations with different indexes, masks those cases where the permutation returns the same element\n* Compares multiple configurations in each run","metadata":{}},{"cell_type":"code","source":"class MultiElementRankLoss(nn.Module):\n    \"\"\"\n    Loss function that compares the output of the model with the output of the model with a permutation of the elements\n    \"\"\"\n    \n    def __init__(self, margin:float=0.0, number_permutations:int = 1) -> None:\n        super().__init__()\n        self.loss_fn = torch.nn.MarginRankingLoss(margin=margin, reduction = 'none')\n        self.number_permutations = number_permutations\n    \n    def calculate_rank_loss(self,\n                            outputs: torch.Tensor,\n                            config_runtime: torch.Tensor,\n                            config_idxs: torch.Tensor\n                            ):\n        \"\"\"\n        Generates a permutation of the predictions and targets and calculates the loss MarginRankingLoss against the permutation\n        Args:\n            outputs: Tensor of shape (bs, seq_len) with the outputs of the model\n            config_runtime: Tensor of shape (bs, seq_len) with the runtime of the model\n            config_mask: Tensor of shape (bs, seq_len) with 1 in the positions of the elements\n            and 0 in the positions of the padding\n        Returns:\n            loss: Tensor of shape (bs, seq_len) with the loss for each element in the batch\n        \"\"\"\n        bs, num_configs = outputs.shape\n        permutation = torch.randperm(num_configs) \n        permuted_idxs = config_idxs[:, permutation]\n        # We mask those cases where we compare the same configuration\n        config_mask = torch.where(config_idxs != permuted_idxs, 1, 0)\n        permuted_runtime = config_runtime[:, permutation]\n        labels = 2*((config_runtime - permuted_runtime) > 0) -1\n        permuted_output = outputs[:, permutation]\n        loss = self.loss_fn(outputs.view(-1,1), permuted_output.view(-1,1), labels.view(-1,1))\n        loss = loss.view(bs, num_configs) * config_mask\n        return loss.mean()\n                \n    \n    def forward(self,\n                outputs: torch.Tensor,\n                config_runtime: torch.Tensor,\n                config_idxs: torch.Tensor\n                ):\n        loss = 0 \n        for _ in range(self.number_permutations):\n            loss += self.calculate_rank_loss(outputs, config_runtime, config_idxs)\n        return loss/ self.number_permutations","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.228454Z","iopub.execute_input":"2023-09-28T12:14:59.228859Z","iopub.status.idle":"2023-09-28T12:14:59.24137Z","shell.execute_reply.started":"2023-09-28T12:14:59.228827Z","shell.execute_reply":"2023-09-28T12:14:59.240254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"code","source":"\nclass TileTopK(tm.Metric):\n    \n    higher_is_better = True\n    \n    def __init__(self, k:int=5) -> None:\n        super().__init__()\n        self.add_state(\"runtimes\", default=[], dist_reduce_fx=None)\n        self.k = k\n        \n    def update(self, preds: torch.Tensor, target: torch.Tensor, config_attn_mask:torch.Tensor) -> None:\n        \"\"\"\n        Update the metric state\n        Args:\n            preds: Tensor of shape (bs, seq_len) with the predicted runtimes orders\n            target: Tensor of shape (bs, seq_len) with the target runtimes\n            config_attn_mask: Tensor of shape (bs, seq_len) with 1 in the positions of the elements\n        \"\"\"\n        best_runtimes = torch.where(config_attn_mask==1, target, torch.tensor(float('inf'))).min(1).values\n        masked_preds = torch.where(config_attn_mask==1, preds, torch.tensor(float('inf')))\n        pred_bottomk_indices = torch.topk(masked_preds, k=self.k, largest=False).indices\n        bs = preds.shape[0]\n        bottom_k_positions = torch.stack([torch.arange(bs).repeat_interleave(self.k).to(config_attn_mask.device), pred_bottomk_indices.view(-1)])\n        predicted_runtimes = target[bottom_k_positions[0], bottom_k_positions[1]].view(bs,self.k)\n        best_predicted_runtimes = predicted_runtimes.min(1).values\n        self.runtimes.append(best_predicted_runtimes/ best_runtimes)\n        \n    def compute(self) -> torch.Tensor:\n        return (2-torch.cat(self.runtimes)).mean()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.243237Z","iopub.execute_input":"2023-09-28T12:14:59.243626Z","iopub.status.idle":"2023-09-28T12:14:59.257161Z","shell.execute_reply.started":"2023-09-28T12:14:59.243592Z","shell.execute_reply":"2023-09-28T12:14:59.256215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\nModified version of ðŸ¤— Bert implementation to take in to account [Graph Attention](https://arxiv.org/abs/1710.10903)\n* Removed the parts corresponding to Cross-attention\n* Made layer_head_mask the same for all layers, heads\n* The Head mask corresponds to the edge adjacency \n","metadata":{}},{"cell_type":"code","source":"# Modified from https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py\nclass BertEncoder(nn.Module):\n    def __init__(self, config:GraphConfig):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n\n        for i, layer_module in enumerate(self.layer):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask #DONE: Same Head Mask for all layers\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs,  output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    all_hidden_states,\n                    all_self_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=None,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=None,\n        )\n        \n        \nclass BertLayer(nn.Module):\n    def __init__(self, config:GraphConfig):\n        super().__init__()\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n        layer_output = apply_chunking_to_forward(\n            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n        )\n        outputs = (layer_output,) + outputs\n\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n    \nclass BertIntermediate(nn.Module):\n    def __init__(self, config:GraphConfig):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n    \nclass BertOutput(nn.Module):\n    def __init__(self, config:GraphConfig):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n    \nclass BertAttention(nn.Module):\n    def __init__(self, config:GraphConfig, position_embedding_type=None):\n        super().__init__()\n        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n    \n    \nclass BertSelfAttention(nn.Module):\n    def __init__(self, config:GraphConfig, position_embedding_type=None):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\n                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({config.num_attention_heads})\"\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = position_embedding_type or getattr(\n            config, \"position_embedding_type\", \"absolute\"\n        )\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n\n\n    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        \n        mixed_query_layer = self.query(hidden_states)\n        key_layer = self.transpose_for_scores(self.key(hidden_states))\n        value_layer = self.transpose_for_scores(self.value(hidden_states))\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n            position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask #DONE: Same Head Mask for all Heads\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n\n        return outputs\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config:GraphConfig):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n    \n    \nclass NodeEncoder(nn.Module):\n    \n    def __init__(self, config:GraphConfig):\n        super().__init__()\n        self.node_opcode_embeddings = nn.Embedding(NODE_OP_CODES+1 , config.embedding_size, padding_idx=NODE_OP_CODES)\n        self.linear = nn.Linear(NODE_FEATS, config.embedding_size, bias=False)\n        self.layer_norm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n        \n        \n    def forward(self,\n                node_opcode: torch.Tensor,\n                node_feat: torch.Tensor\n                ) -> torch.Tensor:\n        opcode_embeddings = self.node_opcode_embeddings(node_opcode) \n        node_feats =  self.linear(node_feat)\n        features = opcode_embeddings + node_feats\n        features = self.layer_norm(features)\n        return features\n    \n    \nclass BertNodeEncoder(nn.Module):\n    \n    def __init__(self, config:GraphConfig) -> None:\n        super().__init__()\n        self.config = config\n        self.node_embeddings = NodeEncoder(config)\n        self.node_encoder = BertEncoder(config)\n        \n    def forward(self,\n                node_opcode: torch.Tensor,\n                node_feat: torch.Tensor,\n                edges_adjecency: torch.Tensor,\n                node_attn_mask: torch.Tensor\n                ):\n        node_embeddings = self.node_embeddings(node_opcode, node_feat)\n        node_attn_mask = node_attn_mask.unsqueeze(1).unsqueeze(-1)\n        node_encoder_outputs = self.node_encoder(node_embeddings,\n                                                 attention_mask=node_attn_mask,\n                                                 head_mask=edges_adjecency.unsqueeze(0).repeat(self.config.num_hidden_layers, 1, 1, 1).unsqueeze(2),\n                                                 output_attentions=True)\n        return node_encoder_outputs\n    \ndef transform_node_positional_embeddings(embeddings_output:torch.Tensor,\n                                         node_config_ids:torch.Tensor,\n                                         num_nodes:int\n                                         ) -> torch.Tensor:\n    bs, num_configs, _, dim = embeddings_output.shape\n    idxs = node_config_ids.unsqueeze(1).repeat(1,num_configs,1)\n    zeros = torch.zeros(bs, num_configs, num_nodes, dim, device=embeddings_output.device, dtype=embeddings_output.dtype)\n    idxs = idxs.unsqueeze(-1).repeat(1,1,1,dim)\n    zeros.scatter_reduce_(2, idxs, embeddings_output, reduce='sum')\n    return zeros\n\nclass NodeFeatEmbeddings(nn.Module):\n    def __init__(self, config:GraphConfig):\n        super().__init__()\n        self.config = config\n        self.node_feat_embeddings = nn.Linear(NODE_CONFIG_FEATS + CONFIG_FEATS, config.embedding_size, bias=False)\n        self.layer_norm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n        \n    def forward(self, node_config_feat: torch.Tensor, node_config_ids: torch.Tensor, num_nodes:int) -> torch.Tensor:\n        node_config_feat_embeddings = self.node_feat_embeddings(node_config_feat)\n        node_config_feat_embeddings = self.layer_norm(node_config_feat_embeddings)\n        node_config_feat_embeddings = transform_node_positional_embeddings(node_config_feat_embeddings, node_config_ids, num_nodes)\n        return node_config_feat_embeddings\n        \n    \nclass BertGraphEncoder(nn.Module):\n    def __init__(self, config:GraphConfig) -> None:\n        super().__init__()\n        self.config = config\n        self.node_embeddings = NodeEncoder(config)\n        self.node_encoder = BertEncoder(config)\n        self.node_feat_embeddings = NodeFeatEmbeddings(config)\n        \n    def forward(self,\n                node_opcode: torch.Tensor, # (bs, num_nodes)\n                node_feat: torch.Tensor, # (bs, num_nodes, num_node_feats)\n                edges_adjecency: torch.Tensor, # (bs, num_nodes, num_nodes)\n                node_attn_mask: torch.Tensor, # (bs, num_nodes)\n                node_config_feat: torch.Tensor, # (bs, num_configs, num_config_nodes, num_node_feats)\n                node_config_ids: torch.Tensor, # (bs, num_configs, num_config_nodes)\n                ):\n        bs, num_nodes = node_opcode.shape\n        num_configs = node_config_feat.shape[1]\n        node_embeddings = self.node_embeddings(node_opcode, node_feat)\n        node_config_feat_embeddings = self.node_feat_embeddings(node_config_feat, node_config_ids, num_nodes)\n        \n        node_embeddings = node_embeddings.unsqueeze(1).repeat(1, num_configs, 1, 1)\n        node_embeddings += node_config_feat_embeddings\n        node_attn_mask = node_attn_mask.unsqueeze(1).repeat(1, num_configs, 1)\n        node_embeddings = node_embeddings.reshape(bs *num_configs, num_nodes, -1)\n        node_attn_mask = node_attn_mask.reshape(bs *num_configs, num_nodes)\n        node_attn_mask = node_attn_mask.unsqueeze(1).unsqueeze(-1)\n        edges_adjecency = edges_adjecency.unsqueeze(1).repeat(1, num_configs, 1, 1).reshape(bs *num_configs, num_nodes, num_nodes)\n        edges_adjecency = edges_adjecency.unsqueeze(1)\n        \n\n        node_encoder_outputs = self.node_encoder(node_embeddings,\n                                                 attention_mask=node_attn_mask,\n                                                 head_mask=edges_adjecency,\n                                                 output_attentions=True)\n        \n        return node_encoder_outputs.last_hidden_state.reshape(bs, num_configs, num_nodes, -1)\n    \n    \nclass GraphEncoder(nn.Module):\n    \n    config_class = GraphConfig\n    \n    def __init__(self, config:GraphConfig):\n        super().__init__()\n        self.config = config\n        self.node_encoder = BertGraphEncoder(config)\n        self.head = nn.Linear(config.hidden_size, 1)\n        self.loss_fn = MultiElementRankLoss(margin=config.margin, number_permutations=config.number_permutations)\n        \n        \n    def forward(self,\n                node_opcode: torch.Tensor, # (bs, num_nodes)\n                node_feat: torch.Tensor, # (bs, num_nodes, num_node_feats)\n                edges_adjecency: torch.Tensor, # (bs, num_nodes, num_nodes)\n                node_attn_mask: torch.Tensor, # (bs, num_nodes)\n                node_config_feat: torch.Tensor, # (bs, num_configs, num_config_nodes, num_node_feats)\n                node_config_ids: torch.Tensor, # (bs, num_configs, num_config_nodes)\n                config_idxs: Optional[torch.Tensor] = None, # (bs, num_configs)\n                config_runtime: Optional[torch.Tensor] = None,):\n        \n        last_hidden_state = self.node_encoder(node_opcode,\n                                    node_feat,\n                                    edges_adjecency,\n                                    node_attn_mask,\n                                    node_config_feat,\n                                    node_config_ids)\n        \n        output = self.head(last_hidden_state[:,:,0]).squeeze(-1)\n        outputs = {'outputs': output, 'order': torch.argsort(output, dim=1)}\n        if config_runtime is not None:\n            loss = 0\n            loss += self.loss_fn(output, config_runtime, config_idxs)\n            outputs['loss'] = loss\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.258868Z","iopub.execute_input":"2023-09-28T12:14:59.259236Z","iopub.status.idle":"2023-09-28T12:14:59.328832Z","shell.execute_reply.started":"2023-09-28T12:14:59.259181Z","shell.execute_reply":"2023-09-28T12:14:59.32791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LightningWrapper(pl.LightningModule):\n    def __init__(self, model:nn.Module):\n        super().__init__()\n        self.model = model\n        self.topk = TileTopK()\n        \n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        outputs = self.model(**batch)\n        return outputs['loss']\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self.model(**batch)\n        loss = outputs['loss']\n        self.log(\"val_loss\", loss, prog_bar=True)\n        config_attn_mask = torch.ones_like(batch['config_runtime'], device=batch['config_runtime'].device)\n        self.topk.update(outputs['outputs'], batch['config_runtime'], config_attn_mask)\n        return loss\n    \n    def on_validation_end(self) -> None:\n        topk = self.topk.compute()\n        self.print(f\"topk {topk:.3f}\")\n        self.topk.reset()\n        return super().on_validation_end()\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.model.loss(y_hat, y)\n        self.log(\"test_loss\", loss, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.trainer.model.parameters(), lr=1e-3)\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.331439Z","iopub.execute_input":"2023-09-28T12:14:59.33169Z","iopub.status.idle":"2023-09-28T12:14:59.345803Z","shell.execute_reply.started":"2023-09-28T12:14:59.331668Z","shell.execute_reply":"2023-09-28T12:14:59.34488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"config_kwargs = dict(hidden_size= 128,\n    num_attention_heads= 4,\n    num_hidden_layers= 2,\n    intermediate_size= 64,\n    gradient_checkpointing= True,\n    margin= 0.1,\n    number_permutations= 4,\n    )","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.347984Z","iopub.execute_input":"2023-09-28T12:14:59.34848Z","iopub.status.idle":"2023-09-28T12:14:59.355524Z","shell.execute_reply.started":"2023-09-28T12:14:59.348446Z","shell.execute_reply":"2023-09-28T12:14:59.35458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = GraphConfig(**config_kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.357146Z","iopub.execute_input":"2023-09-28T12:14:59.35755Z","iopub.status.idle":"2023-09-28T12:14:59.369607Z","shell.execute_reply.started":"2023-09-28T12:14:59.357518Z","shell.execute_reply":"2023-09-28T12:14:59.368588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = GraphEncoder(config)\nmodel = LightningWrapper(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.37104Z","iopub.execute_input":"2023-09-28T12:14:59.371458Z","iopub.status.idle":"2023-09-28T12:14:59.386828Z","shell.execute_reply.started":"2023-09-28T12:14:59.371426Z","shell.execute_reply":"2023-09-28T12:14:59.385993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tile_df","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.388238Z","iopub.execute_input":"2023-09-28T12:14:59.388564Z","iopub.status.idle":"2023-09-28T12:14:59.409649Z","shell.execute_reply.started":"2023-09-28T12:14:59.388534Z","shell.execute_reply":"2023-09-28T12:14:59.408685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = tile_df.query(\"split == 'train'\").reset_index(drop=True)\nvalid_df = tile_df.query(\"split == 'valid'\").reset_index(drop=True)\ntrain_dataset = TileDataset(train_df, num_configs=24)\nvalid_dataset = TileDataset(valid_df, num_configs=24)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.411164Z","iopub.execute_input":"2023-09-28T12:14:59.41156Z","iopub.status.idle":"2023-09-28T12:14:59.434234Z","shell.execute_reply.started":"2023-09-28T12:14:59.411527Z","shell.execute_reply":"2023-09-28T12:14:59.433302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=8, num_workers=2, shuffle=True, persistent_workers=True)\nvalid_dataloader = DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=8, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.436752Z","iopub.execute_input":"2023-09-28T12:14:59.437316Z","iopub.status.idle":"2023-09-28T12:14:59.443Z","shell.execute_reply.started":"2023-09-28T12:14:59.437283Z","shell.execute_reply":"2023-09-28T12:14:59.441822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_config = dict(\n    max_epochs= 50,\n    precision= 32,\n    gradient_clip_val= 1.0,\n    accumulate_grad_batches= 4,\n    check_val_every_n_epoch= 10)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.444533Z","iopub.execute_input":"2023-09-28T12:14:59.445245Z","iopub.status.idle":"2023-09-28T12:14:59.454092Z","shell.execute_reply.started":"2023-09-28T12:14:59.445212Z","shell.execute_reply":"2023-09-28T12:14:59.453044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.set_float32_matmul_precision(\"medium\")\ntrainer = pl.Trainer(**trainer_config,)\ntrainer.fit(model, train_dataloader, valid_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:14:59.457658Z","iopub.execute_input":"2023-09-28T12:14:59.458151Z","iopub.status.idle":"2023-09-28T12:15:31.478122Z","shell.execute_reply.started":"2023-09-28T12:14:59.458116Z","shell.execute_reply":"2023-09-28T12:15:31.468811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:15:31.481967Z","iopub.execute_input":"2023-09-28T12:15:31.482368Z","iopub.status.idle":"2023-09-28T12:15:31.566492Z","shell.execute_reply.started":"2023-09-28T12:15:31.482334Z","shell.execute_reply":"2023-09-28T12:15:31.564566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = 'test'\ntest_tile_df = tile_df.query(\"split == @split\").reset_index(drop=True)\ntest_tile_ds = TileDataset(test_tile_df, num_configs=-1)\ncollate_fn = LayoutCollator(64, targets=split!=\"test\")\ntest_dataloader = DataLoader(test_tile_ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:15:31.574727Z","iopub.execute_input":"2023-09-28T12:15:31.575145Z","iopub.status.idle":"2023-09-28T12:15:31.622616Z","shell.execute_reply.started":"2023-09-28T12:15:31.575108Z","shell.execute_reply":"2023-09-28T12:15:31.621574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nmodel = model.eval()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:15:31.624667Z","iopub.execute_input":"2023-09-28T12:15:31.625039Z","iopub.status.idle":"2023-09-28T12:15:38.07612Z","shell.execute_reply.started":"2023-09-28T12:15:31.625008Z","shell.execute_reply":"2023-09-28T12:15:38.07476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunk_batch(batch, start_idx, end_idx):\n    output = {k:batch[k] for k in ['node_opcode', 'node_feat', 'edges_adjecency', 'node_attn_mask', 'node_config_ids']}\n    output['node_config_feat'] = batch['node_config_feat'][:, start_idx: end_idx]\n    return output\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:15:38.078011Z","iopub.execute_input":"2023-09-28T12:15:38.078733Z","iopub.status.idle":"2023-09-28T12:15:38.095347Z","shell.execute_reply.started":"2023-09-28T12:15:38.078678Z","shell.execute_reply":"2023-09-28T12:15:38.093734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_order = []\nfor batch in tqdm(test_dataloader):\n    batch.pop('config_idxs')\n    batch = {k: v.to(device) for k, v in batch.items()}\n    num_configs = batch['node_config_feat'].shape[1]\n    # Chunk the configs to avoid OOM errors\n    configs_cut_points = list(range(0,num_configs, 100)) + [num_configs]\n    chunk_order = []\n    for start, end in zip(configs_cut_points, configs_cut_points[1:]):\n        chunked_batch = chunk_batch(batch, start, end)\n        with torch.no_grad():\n            output = model.model(**chunked_batch)\n        chunk_order.extend(output['outputs'].cpu().numpy())\n    pred_order.append(np.argsort(np.concatenate(chunk_order))[:5])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:15:38.097434Z","iopub.execute_input":"2023-09-28T12:15:38.09821Z","iopub.status.idle":"2023-09-28T12:19:09.419044Z","shell.execute_reply.started":"2023-09-28T12:15:38.098178Z","shell.execute_reply":"2023-09-28T12:19:09.417798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idxs_string = [\";\".join(map(str,elem)) for elem in pred_order]\ntest_tile_df['TopConfigs'] = idxs_string\ntest_tile_df = test_tile_df[['ID', 'TopConfigs']]\ntest_tile_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:19:09.425203Z","iopub.execute_input":"2023-09-28T12:19:09.42615Z","iopub.status.idle":"2023-09-28T12:19:09.502792Z","shell.execute_reply.started":"2023-09-28T12:19:09.426102Z","shell.execute_reply":"2023-09-28T12:19:09.501552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/predict-ai-model-runtime/sample_submission.csv')\nsubmission_df = submission_df.query(f\"ID not in {test_tile_df.ID.tolist()}\")\nsubmission_df = pd.concat([test_tile_df, submission_df])\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:19:09.504178Z","iopub.execute_input":"2023-09-28T12:19:09.504633Z","iopub.status.idle":"2023-09-28T12:19:09.695414Z","shell.execute_reply.started":"2023-09-28T12:19:09.504589Z","shell.execute_reply":"2023-09-28T12:19:09.694033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/input/fast-slow-4-dataset-train/torch_geometric-2.3.1-py3-none-any.whl\n!pip install /kaggle/input/fast-slow-4-dataset-train/torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:30:20.205904Z","iopub.execute_input":"2023-10-07T11:30:20.206243Z","iopub.status.idle":"2023-10-07T11:31:30.653952Z","shell.execute_reply.started":"2023-10-07T11:30:20.20622Z","shell.execute_reply":"2023-10-07T11:31:30.65275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install timm\n","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:31:30.65625Z","iopub.execute_input":"2023-10-07T11:31:30.656615Z","iopub.status.idle":"2023-10-07T11:32:03.172189Z","shell.execute_reply.started":"2023-10-07T11:31:30.656581Z","shell.execute_reply":"2023-10-07T11:32:03.17105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\nfrom timm.scheduler import  CosineLRScheduler\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm \n\nimport sklearn,sklearn.model_selection\nimport torch\nfrom torch import nn\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv,SAGEConv\nfrom torch_geometric.datasets import Planetoid\nfrom torch.utils.data import DataLoader, Dataset\n#from timm.scheduler import CosineLRScheduler\nimport matplotlib.pyplot as plt\ndevice = 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:32:03.174025Z","iopub.execute_input":"2023-10-07T11:32:03.174364Z","iopub.status.idle":"2023-10-07T11:32:09.179341Z","shell.execute_reply.started":"2023-10-07T11:32:03.174337Z","shell.execute_reply":"2023-10-07T11:32:09.178325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_df(directory):\n    splits = [\"test\"]\n    dfs = dict()\n    \n    for split in splits:\n        path = os.path.join(directory, split)\n        files = os.listdir(path)\n        list_df = []\n        \n        for file in files:\n            d = dict(np.load(os.path.join(path,file)))\n            d['file'] = file\n            list_df.append(d)\n        dfs[split] = pd.DataFrame.from_dict(list_df)\n    return dfs\nlayout_xla_random = load_df(\"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout/xla/random/\")\nlayout_xla_default = load_df(\"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout/xla/default/\")\nlayout_nlp_default = load_df(\"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout/nlp/default/\")\nlayout_nlp_random = load_df(\"/kaggle/input/predict-ai-model-runtime/npz_all/npz/layout/nlp/random/\")","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:32:09.181235Z","iopub.execute_input":"2023-10-07T11:32:09.181747Z","iopub.status.idle":"2023-10-07T11:32:12.802641Z","shell.execute_reply.started":"2023-10-07T11:32:09.181713Z","shell.execute_reply":"2023-10-07T11:32:12.801118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TileDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        config_feat = torch.tensor(row['node_config_feat'].astype(np.float32))\n        node_feat = torch.tensor(row['node_feat'].astype(np.float32))\n        node_opcode = torch.tensor(row['node_opcode'].astype(np.int64))\n        edge_index = torch.tensor(np.swapaxes(row['edge_index'],0,1).astype(np.int64))\n        target = (row['config_runtime']).astype(np.float32)\n        # minmax scale the target, we only care about order\n        target = (target-min(target))/(max(target) -min(target))\n        target = torch.tensor(target)\n        return config_feat,node_feat,node_opcode,edge_index,target\n    \nclass SimpleModel(torch.nn.Module):\n    def __init__(self, hidden_channels, graph_feats, hidden_dim):\n        super().__init__()\n        op_embedding_dim = 4 # I choose 4-dimensional embedding\n        self.embedding = torch.nn.Embedding(120, #120 different op-codes\n                                            op_embedding_dim,\n                                           )\n        assert len(hidden_channels)>0\n        in_channels = op_embedding_dim+140\n        self.convs = torch.nn.ModuleList()\n        last_dim = hidden_channels[0]\n        self.convs.append(GCNConv(in_channels, hidden_channels[0]))\n        for i in range(len(hidden_channels)-1):\n            self.convs.append(GCNConv(hidden_channels[i], hidden_channels[i+1]))\n            last_dim = hidden_channels[i+1]\n        self.convs.append(GCNConv(last_dim, graph_feats))\n        \n        self.dense = torch.nn.Sequential(nn.Linear(82, 64),\n                                         nn.ReLU(),\n                                         nn.Linear(64, 64),\n                                         nn.ReLU(),\n                                         nn.Linear(64, 1),\n                                        )\n    \n    def forward(self, x_cfg: Tensor,x_feat: Tensor, x_op: Tensor, edge_index: Tensor) -> Tensor:\n        \n        #get graph features\n        x_cfg = x_cfg.mean(dim=1)\n        #print(x_cfg.shape)\n        x = torch.concat([x_feat,self.embedding(x_op)],dim = 1)\n        #pass though conv layers\n        for conv in self.convs:\n            x = conv(x, edge_index).relu()\n        # get 1d graph embedding using average pooling\n        x_graph = torch.mean(x,0)\n        \n        \n        #put graph data into config data\n        x = torch.concat([x_cfg,x_graph.repeat((len(x_cfg),1))],axis=1) #torch.Size([10528, 225])\n        #put into dense nn\n        #print(x.shape)\n        x = torch.flatten(self.dense(x))\n        return x\n\nmodel = SimpleModel(hidden_channels = [16,32,16,48],graph_feats = 64,hidden_dim=64).to(device)\nclass SimpleModel2(torch.nn.Module):\n    def __init__(self, hidden_channels, graph_feats, hidden_dim):\n        super().__init__()\n        op_embedding_dim = 4 # I choose 4-dimensional embedding\n        self.embedding = torch.nn.Embedding(120, #120 different op-codes\n                                            op_embedding_dim,\n                                           )\n        assert len(hidden_channels)>0\n        in_channels = op_embedding_dim+140\n        self.convs = torch.nn.ModuleList()\n        last_dim = hidden_channels[0]\n        self.convs.append(SAGEConv(in_channels, hidden_channels[0]))\n        for i in range(len(hidden_channels)-1):\n            self.convs.append(SAGEConv(hidden_channels[i], hidden_channels[i+1]))\n            last_dim = hidden_channels[i+1]\n        self.convs.append(SAGEConv(last_dim, graph_feats))\n        \n        self.dense = torch.nn.Sequential(nn.Linear(82, 64),\n                                         nn.ReLU(),\n                                         nn.Linear(64, 64),\n                                         nn.ReLU(),\n                                         nn.Linear(64, 1),\n                                        )\n    \n    def forward(self, x_cfg: Tensor,x_feat: Tensor, x_op: Tensor, edge_index: Tensor) -> Tensor:\n        \n        #get graph features\n        x_cfg = x_cfg.mean(dim=1)\n        #print(x_cfg.shape)\n        x = torch.concat([x_feat,self.embedding(x_op)],dim = 1)\n        #pass though conv layers\n        for conv in self.convs:\n            x = conv(x, edge_index).relu()\n        # get 1d graph embedding using average pooling\n        x_graph = torch.mean(x,0)\n        \n        \n        #put graph data into config data\n        x = torch.concat([x_cfg,x_graph.repeat((len(x_cfg),1))],axis=1) #torch.Size([10528, 225])\n        #put into dense nn\n        #print(x.shape)\n        x = torch.flatten(self.dense(x))\n        return x\n\nmodel2 = SimpleModel2(hidden_channels = [16,32,16,48],graph_feats = 64,hidden_dim=64).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:32:19.540784Z","iopub.execute_input":"2023-10-07T11:32:19.541165Z","iopub.status.idle":"2023-10-07T11:32:19.597713Z","shell.execute_reply.started":"2023-10-07T11:32:19.541138Z","shell.execute_reply":"2023-10-07T11:32:19.596839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TileDataset(layout_xla_default[\"test\"])\ntile_xla_predictions = [[] for i in range(len(dataset))]\nfor fold in range(5):\n    model.load_state_dict(torch.load(f'/kaggle/input/fast-slow-sep/xla_defalut/layout_xla_default_best_model_{fold}.pth',map_location=torch.device('cpu') ))\n    model.eval()\n    pbar = tqdm(range(len(dataset)))\n    for i in pbar:\n        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n\n        out = model(cfg_ft,nd_ft,nd_op,ind)\n        tile_xla_predictions[i].append(out.cpu().detach().numpy())\ntile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:-1] for pred in tile_xla_predictions]\ntile_xla_predictions[0]\n#sub = submission_df\n#sub = pd.read_csv('/kaggle/input/predict-ai-model-runtime/sample_submission.csv')\nfor i,filename in enumerate(layout_xla_random[\"test\"]['file'].values):\n    id = 'layout:xla:default:' +filename[:-4]\n    print(id)\n    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\nsub.to_csv('submission.csv',index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:32:37.766681Z","iopub.execute_input":"2023-10-07T11:32:37.767096Z","iopub.status.idle":"2023-10-07T11:32:44.899003Z","shell.execute_reply.started":"2023-10-07T11:32:37.767064Z","shell.execute_reply":"2023-10-07T11:32:44.898192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"dataset = TileDataset(layout_xla_random[\"test\"])\ntile_xla_predictions = [[] for i in range(len(dataset))]\nfor fold in range(5):\n    model.load_state_dict(torch.load(f'/kaggle/input/fast-slow-sep/xla_random/layout_xla_default_best_model_{fold}.pth',map_location=torch.device('cpu') ))\n    model.eval()\n    pbar = tqdm(range(len(dataset)))\n    for i in pbar:\n        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n\n        out = model(cfg_ft,nd_ft,nd_op,ind)\n        tile_xla_predictions[i].append(out.cpu().detach().numpy())\ntile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:-1] for pred in tile_xla_predictions]\ntile_xla_predictions[0]\n\n#sub = pd.read_csv('/kaggle/input/predict-ai-model-runtime/sample_submission.csv')\nfor i,filename in enumerate(layout_xla_random[\"test\"]['file'].values):\n    id = 'layout:xla:random:' +filename[:-4]\n    print(id)\n    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\nsub.to_csv('submission.csv',index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:32:57.018869Z","iopub.execute_input":"2023-10-07T11:32:57.019217Z","iopub.status.idle":"2023-10-07T11:33:05.260565Z","shell.execute_reply.started":"2023-10-07T11:32:57.019189Z","shell.execute_reply":"2023-10-07T11:33:05.259591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TileDataset(layout_nlp_default[\"test\"])\ntile_xla_predictions = [[] for i in range(len(dataset))]\nfor fold in range(5):\n    model.load_state_dict(torch.load(f'/kaggle/input/fast-slow-nlp-v3/nlp_default/layout_xla_default_best_model_{fold}.pth',map_location=torch.device('cpu') ))\n    model.eval()\n    pbar = tqdm(range(len(dataset)))\n    for i in pbar:\n        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n        out = model(cfg_ft,nd_ft,nd_op,ind) \n        tile_xla_predictions[i].append(out.cpu().detach().numpy())\ntile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:-1] for pred in tile_xla_predictions]\ntile_xla_predictions[0]\n\n#sub = pd.read_csv('/kaggle/input/predict-ai-model-runtime/sample_submission.csv')\nfor i,filename in enumerate(layout_nlp_default[\"test\"]['file'].values):\n    id = 'layout:nlp:default:' +filename[:-4]\n    print(id)\n    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\nsub.to_csv('submission.csv',index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:33:07.909638Z","iopub.execute_input":"2023-10-07T11:33:07.910002Z","iopub.status.idle":"2023-10-07T11:33:10.100618Z","shell.execute_reply.started":"2023-10-07T11:33:07.909974Z","shell.execute_reply":"2023-10-07T11:33:10.099584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TileDataset(layout_nlp_random[\"test\"])\ntile_xla_predictions = [[] for i in range(len(dataset))]\nfor fold in range(2):\n    model.load_state_dict(torch.load(f'/kaggle/input/fast-slow-sep/nlp_random/layout_xla_default_best_model_{fold}.pth',map_location=torch.device('cpu') ))\n    model.eval()\n    \n    pbar = tqdm(range(len(dataset)))\n    for i in pbar:\n        cfg_ft,nd_ft,nd_op,ind,target = dataset[i]\n        cfg_ft,nd_ft,nd_op,ind,target = cfg_ft.to(device),nd_ft.to(device),nd_op.to(device),ind.to(device),target.to(device)\n\n        out = model(cfg_ft,nd_ft,nd_op,ind) \n        tile_xla_predictions[i].append(out.cpu().detach().numpy())\ntile_xla_predictions = [np.argsort(np.mean(pred,axis=0))[:-1] for pred in tile_xla_predictions]\ntile_xla_predictions[0]\n\n#sub = pd.read_csv('/kaggle/input/predict-ai-model-runtime/sample_submission.csv')\nfor i,filename in enumerate(layout_nlp_random[\"test\"]['file'].values):\n    id = 'layout:nlp:random:' +filename[:-4]\n    print(id)\n    sub.loc[sub.ID == id,'TopConfigs'] = ';'.join(tile_xla_predictions[i].astype(str))\nsub.to_csv('submission.csv',index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2023-10-07T11:33:30.826689Z","iopub.execute_input":"2023-10-07T11:33:30.827135Z","iopub.status.idle":"2023-10-07T11:33:31.853802Z","shell.execute_reply.started":"2023-10-07T11:33:30.827099Z","shell.execute_reply":"2023-10-07T11:33:31.852812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}